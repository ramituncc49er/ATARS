{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4487a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747906d4",
   "metadata": {},
   "source": [
    "### 2-way (binary) evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping to convert full label names to single-letter codes\n",
    "label_map = {\n",
    "    'High': 'H',\n",
    "    'Medium': 'M',\n",
    "    'Low': 'L',\n",
    "    'None': 'N'\n",
    "}\n",
    "\n",
    "# Custom scoring table mapping (Gold, System) -> {TP, TN, FP, FN}\n",
    "score_mapping = {\n",
    "    ('H', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'L'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('H', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('M', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'L'): {'TP': 0.25, 'TN': 0.25,'FP': 0.25,'FN': 0.25},\n",
    "    ('M', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('L', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('L', 'M'): {'TP': 0.25, 'TN': 0.25,'FP': 0.25,'FN': 0.25},\n",
    "    ('L', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('L', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'M'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "}\n",
    "\n",
    "'''score_mapping = {\n",
    "    ('H', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'L'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('H', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('M', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'L'): {'TP': 0,    'TN': 0.5, 'FP': 0,  'FN': 0.5},\n",
    "    ('M', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('L', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('L', 'M'): {'TP': 0.5,  'TN': 0,   'FP': 0.5,'FN': 0},\n",
    "    ('L', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('L', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'M'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "}\n",
    "\n",
    "score_mapping = {\n",
    "    ('H', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'L'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('H', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('M', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'L'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('M', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('L', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('L', 'M'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('L', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('L', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'M'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "}\n",
    "\n",
    "score_mapping = {\n",
    "    ('H', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('H', 'L'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('H', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('M', 'H'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'M'): {'TP': 1,    'TN': 0,   'FP': 0,  'FN': 0},\n",
    "    ('M', 'L'): {'TP': 0.5,  'TN': 0.5, 'FP': 0.5,'FN': 0.5},\n",
    "    ('M', 'N'): {'TP': 0,    'TN': 0,   'FP': 0,  'FN': 1},\n",
    "    ('L', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('L', 'M'): {'TP': 0.5,  'TN': 0.5, 'FP': 0.5,'FN': 0.5},\n",
    "    ('L', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('L', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'H'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'M'): {'TP': 0,    'TN': 0,   'FP': 1,  'FN': 0},\n",
    "    ('N', 'L'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "    ('N', 'N'): {'TP': 0,    'TN': 1,   'FP': 0,  'FN': 0},\n",
    "}'''\n",
    "\n",
    "gold_df = pd.read_excel('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/ground_truth_w_split_abstractive_rag.xlsx')\n",
    "sys_df = pd.read_excel('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/system_user_profiles_restaurants_reviews_match_rag.xlsx')\n",
    "\n",
    "if len(gold_df) != len(sys_df):\n",
    "    raise ValueError(\"The number of records in the gold file does not match the system file!\")\n",
    "\n",
    "total_TP = 0.0\n",
    "total_FP = 0.0\n",
    "total_FN = 0.0\n",
    "total_TN = 0.0\n",
    "\n",
    "record_logs = []\n",
    "gold_labels_list = []\n",
    "sys_labels_list = []\n",
    "\n",
    "print(\"Processing records and computing scores...\\n\")\n",
    "\n",
    "for idx, (gold_row, sys_row) in enumerate(zip(gold_df.itertuples(index=False), sys_df.itertuples(index=False))):\n",
    "    # Retrieve the A_prime field from each file\n",
    "    gold_val = gold_row.A_prime   # e.g., \"A' = ('amazing tour de bier', 'Low')\"\n",
    "    sys_val = sys_row.A_prime     # e.g., \"A' = [('amazing tour de bier', 'None')]\"\n",
    "    \n",
    "    # Remove the \"A' = \" prefix if present\n",
    "    if gold_val.startswith(\"A' =\"):\n",
    "        gold_val = gold_val.replace(\"A' = \", \"\", 1).strip()\n",
    "    if sys_val.startswith(\"A' =\"):\n",
    "        sys_val = sys_val.replace(\"A' = \", \"\", 1).strip()\n",
    "    \n",
    "    # Convert the string representations to actual Python objects\n",
    "    try:\n",
    "        gold_tuple = ast.literal_eval(gold_val)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing gold A_prime at record {idx}: {e}\")\n",
    "        \n",
    "    try:\n",
    "        sys_list = ast.literal_eval(sys_val)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing system A_prime at record {idx}: {e}\")\n",
    "        \n",
    "    # Extract the atypical aspect phrase and label from gold data\n",
    "    gold_aspect, gold_label_text = gold_tuple\n",
    "    \n",
    "    # For system data, ensure that there is at least one prediction\n",
    "    if not sys_list:\n",
    "        raise ValueError(f\"No system prediction found at record {idx}\")\n",
    "    \n",
    "    # Ensure sys_list is either a tuple directly or a list containing tuples\n",
    "    if isinstance(sys_list, tuple) and len(sys_list) == 2:\n",
    "        sys_aspect, sys_label_text = sys_list\n",
    "    elif isinstance(sys_list, list) and len(sys_list) > 0 and isinstance(sys_list[0], tuple) and len(sys_list[0]) == 2:\n",
    "        sys_aspect, sys_label_text = sys_list[0]  # Unpack first element if it's a list\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected format in system A_prime at record {idx}: {sys_list}\")\n",
    "    \n",
    "    if gold_aspect != sys_aspect:\n",
    "        raise ValueError(f\"Aspect phrase mismatch at record {idx}: Gold aspect '{gold_aspect}' vs System aspect '{sys_aspect}'\")\n",
    "        \n",
    "    gold_labels_list.append(gold_label_text)\n",
    "    sys_labels_list.append(sys_label_text)\n",
    "\n",
    "    gold_code = label_map.get(gold_label_text, gold_label_text)\n",
    "    sys_code = label_map.get(sys_label_text, sys_label_text)\n",
    "    \n",
    "    try:\n",
    "        scores = score_mapping[(gold_code, sys_code)]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Invalid label pair at record {idx}: Gold '{gold_code}', System '{sys_code}'\")\n",
    "\n",
    "    record_logs.append({\n",
    "        \"Record\": idx,\n",
    "        \"Atypical Aspect\": gold_aspect,\n",
    "        \"Gold Label\": gold_label_text,\n",
    "        \"Gold Code\": gold_code,\n",
    "        \"System Label\": sys_label_text,\n",
    "        \"Sys Code\": sys_code,\n",
    "        \"TP\": scores['TP'],\n",
    "        \"TN\": scores['TN'],\n",
    "        \"FP\": scores['FP'],\n",
    "        \"FN\": scores['FN']\n",
    "    })\n",
    "    \n",
    "    print(f\"Record {idx}:\")\n",
    "    print(f\"  Aspect Phrase: '{gold_aspect}'\")\n",
    "    print(f\"  Gold Label: '{gold_label_text}' (converted to '{gold_code}')\")\n",
    "    print(f\"  System Label: '{sys_label_text}' (converted to '{sys_code}')\")\n",
    "    print(f\"  Assigned Scores: {scores}\\n\")\n",
    "    \n",
    "    total_TP += scores['TP']\n",
    "    total_FP += scores['FP']\n",
    "    total_FN += scores['FN']\n",
    "    total_TN += scores['TN']\n",
    "\n",
    "precision = (total_TP / (total_TP + total_FP) * 100) if (total_TP + total_FP) > 0 else 0\n",
    "recall = (total_TP / (total_TP + total_FN) * 100) if (total_TP + total_FN) > 0 else 0\n",
    "f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "accuracy = ((total_TP + total_TN) / (total_TP + total_TN + total_FP + total_FN) * 100) if (total_TP + total_TN + total_FP + total_FN) > 0 else 0\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Total TP\": total_TP,\n",
    "    \"Total FP\": total_FP,\n",
    "    \"Total FN\": total_FN,\n",
    "    \"Total TN\": total_TN,\n",
    "    \"Precision\": round(precision, 2),\n",
    "    \"Recall\": round(recall, 2),\n",
    "    \"F1 Score\": round(f1_score, 2),\n",
    "    \"Accuracy\": round(accuracy, 2)\n",
    "}\n",
    "\n",
    "print(\"Aggregated Confusion Matrix Totals (Custom Scoring):\")\n",
    "print(f\"  Total TP: {total_TP}\")\n",
    "print(f\"  Total FP: {total_FP}\")\n",
    "print(f\"  Total FN: {total_FN}\")\n",
    "print(f\"  Total TN: {total_TN}\\n\")\n",
    "\n",
    "print(\"Evaluation Metrics (Custom Scoring):\")\n",
    "print(f\"  Precision: {precision * 100:.1f}\")\n",
    "print(f\"  Recall:    {recall * 100:.1f}\")\n",
    "print(f\"  F1 Score:  {f1_score * 100:.1f}\")\n",
    "print(f\"  Accuracy:  {accuracy * 100:.1f}\")\n",
    "\n",
    "labels_order = [\"None\", \"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "cm = pd.crosstab(\n",
    "    pd.Series(gold_labels_list, name=\"Gold\"),\n",
    "    pd.Series(sys_labels_list, name=\"System\")\n",
    ")\n",
    "\n",
    "cm = cm.reindex(index=labels_order, columns=labels_order, fill_value=0)\n",
    "\n",
    "print(\"\\nStandard Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# -------------------- New: Classification Report --------------------\n",
    "classification_rep = classification_report(\n",
    "    gold_labels_list, sys_labels_list, target_names=labels_order, digits=2, output_dict=True\n",
    ")\n",
    "df_classification_report = pd.DataFrame(classification_rep).transpose()\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "df_records = pd.DataFrame(record_logs)\n",
    "df_evaluation = pd.DataFrame(list(evaluation_metrics.items()), columns=[\"Metric\", \"Value\"])\n",
    "df_confusion = cm.reset_index()  # convert confusion matrix to DataFrame for Excel output\n",
    "\n",
    "output_filename = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/statistics_rag.xlsx\"\n",
    "with pd.ExcelWriter(output_filename) as writer:\n",
    "    df_records.to_excel(writer, sheet_name=\"Record Details\", index=False)\n",
    "    df_evaluation.to_excel(writer, sheet_name=\"Evaluation Metrics\", index=False)\n",
    "    df_confusion.to_excel(writer, sheet_name=\"Confusion Matrix\", index=False)\n",
    "    df_classification_report.to_excel(writer, sheet_name=\"Classification Report\", index=False)\n",
    "\n",
    "print(f\"\\nOutput written to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8b183",
   "metadata": {},
   "source": [
    "### 4-way evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'None': 0, 'Low': 1, 'Medium': 2, 'High': 3}\n",
    "\n",
    "def calculate_sample_standard_deviation(system_label, manual_label):\n",
    "    \"\"\"Calculate the standard deviation between system and manual labels.\"\"\"\n",
    "    values = [system_label, manual_label]\n",
    "    mean_value = sum(values) / len(values)\n",
    "    standard_deviation = (sum((x - mean_value) ** 2 for x in values) / len(values)) ** 0.5\n",
    "    return standard_deviation\n",
    "\n",
    "def calculate_pairwise_absolute_deviation(system_label, manual_label):\n",
    "    \"\"\"Calculate absolute deviation between system and manual labels.\"\"\"\n",
    "    return abs(system_label - manual_label)\n",
    "\n",
    "def calculate_accuracy(system_label, manual_label):\n",
    "    \"\"\"\n",
    "    Calculate accuracy based on proximity between system predictions and manual annotations.\n",
    "    \"\"\"\n",
    "    if system_label == manual_label:\n",
    "        return 1\n",
    "    elif abs(system_label - manual_label) == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def evaluate_performance(data, label_mapping):\n",
    "    \"\"\"\n",
    "    Evaluate absolute deviation, standard deviation, and accuracy\n",
    "    between system and manual labels.\n",
    "    Expects the DataFrame to have 'manual_label' and 'system_label' columns.\n",
    "    \"\"\"\n",
    "    # Map string labels to numeric values\n",
    "    data['system_label_numeric'] = data['system_label'].map(label_mapping)\n",
    "    data['manual_label_numeric'] = data['manual_label'].map(label_mapping)\n",
    "\n",
    "    deviations = []\n",
    "    for _, row in data.iterrows():\n",
    "        sys_val = row['system_label_numeric']\n",
    "        man_val = row['manual_label_numeric']\n",
    "\n",
    "        # Calculate metrics; if either value is NaN, metrics will be NaN\n",
    "        std_dev = calculate_sample_standard_deviation(sys_val, man_val) if pd.notnull(sys_val) and pd.notnull(man_val) else None\n",
    "        abs_dev = calculate_pairwise_absolute_deviation(sys_val, man_val) if pd.notnull(sys_val) and pd.notnull(man_val) else None\n",
    "        acc = calculate_accuracy(sys_val, man_val) if pd.notnull(sys_val) and pd.notnull(man_val) else \"NaN\"\n",
    "\n",
    "        details = f\"Labels: [{sys_val}, {man_val}], StdDev: {std_dev:.3f} \" if std_dev is not None else \"Labels not parsed correctly\"\n",
    "        deviations.append((std_dev, abs_dev, acc, details))\n",
    "\n",
    "    data[['Standard_Deviation', 'Absolute_Deviation', 'Accuracy', 'Deviation_Calculation_Details']] = pd.DataFrame(deviations, index=data.index)\n",
    "    return data\n",
    "\n",
    "def calculate_summary(data):\n",
    "    \"\"\"Calculate summary metrics and return them as a DataFrame.\"\"\"\n",
    "    mean_std_dev = round(data['Standard_Deviation'].mean(), 3)\n",
    "    mean_abs_dev = round(data['Absolute_Deviation'].mean(), 3)\n",
    "    mean_acc = round(data['Accuracy'].mean(), 3)\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Metric': ['Mean Standard Deviation', 'Mean Absolute Deviation', 'Mean Accuracy'],\n",
    "        'Value': [mean_std_dev, mean_abs_dev, mean_acc]\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "def extract_labels(gold_df, sys_df):\n",
    "    \"\"\"\n",
    "    Extract label values from the A_prime column in the gold and system DataFrames.\n",
    "    Returns a DataFrame with columns 'manual_label' and 'system_label'.\n",
    "    \"\"\"\n",
    "    gold_labels_list = []\n",
    "    sys_labels_list = []\n",
    "    \n",
    "    for idx, (gold_row, sys_row) in enumerate(zip(gold_df.itertuples(index=False), sys_df.itertuples(index=False))):\n",
    "        gold_val = getattr(gold_row, 'A_prime')\n",
    "        sys_val = getattr(sys_row, 'A_prime')\n",
    "        \n",
    "        if gold_val.startswith(\"A' =\"):\n",
    "            gold_val = gold_val.replace(\"A' =\", \"\", 1).strip()\n",
    "        if sys_val.startswith(\"A' =\"):\n",
    "            sys_val = sys_val.replace(\"A' =\", \"\", 1).strip()\n",
    "        \n",
    "        try:\n",
    "            gold_tuple = ast.literal_eval(gold_val)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing gold A_prime at record {idx}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            sys_obj = ast.literal_eval(sys_val)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing system A_prime at record {idx}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            gold_aspect, gold_label_text = gold_tuple\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error unpacking gold A_prime at record {idx}: {e}\")\n",
    "        \n",
    "        if isinstance(sys_obj, tuple) and len(sys_obj) == 2:\n",
    "            sys_aspect, sys_label_text = sys_obj\n",
    "        elif isinstance(sys_obj, list) and len(sys_obj) > 0 and isinstance(sys_obj[0], tuple) and len(sys_obj[0]) == 2:\n",
    "            sys_aspect, sys_label_text = sys_obj[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected format in system A_prime at record {idx}: {sys_obj}\")\n",
    "        \n",
    "        if gold_aspect != sys_aspect:\n",
    "            raise ValueError(f\"Aspect phrase mismatch at record {idx}: Gold aspect '{gold_aspect}' vs System aspect '{sys_aspect}'\")\n",
    "        \n",
    "        gold_labels_list.append(gold_label_text)\n",
    "        sys_labels_list.append(sys_label_text)\n",
    "    \n",
    "    return pd.DataFrame({'manual_label': gold_labels_list, 'system_label': sys_labels_list})\n",
    "\n",
    "ground_truth_file = '/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/ground_truth_w_split_abstractive_rag.xlsx'\n",
    "system_file = '/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/system_user_profiles_restaurants_reviews_match_rag.xlsx'\n",
    "\n",
    "df_gold = pd.read_excel(ground_truth_file)\n",
    "df_sys = pd.read_excel(system_file)\n",
    "df_labels = extract_labels(df_gold, df_sys)\n",
    "df_eval = evaluate_performance(df_labels.copy(), label_mapping)\n",
    "summary = calculate_summary(df_eval)\n",
    "\n",
    "print(\"Summary Metrics:\")\n",
    "print(summary)\n",
    "print(\"\\nDetailed Evaluation (first 5 rows):\")\n",
    "print(df_eval.head())\n",
    "\n",
    "with pd.ExcelWriter('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/evaluation_results_rag3.xlsx') as writer:\n",
    "    df_eval.to_excel(writer, sheet_name='Evaluation', index=False)\n",
    "    summary.to_excel(writer, sheet_name='Summary', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
