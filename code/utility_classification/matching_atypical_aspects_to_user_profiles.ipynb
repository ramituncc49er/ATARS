{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045175eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.stats import hmean\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "from openai.error import RateLimitError, APIConnectionError, InvalidRequestError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bca24",
   "metadata": {},
   "source": [
    "#### GPT-4 configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = \"https://rumi-gpt4.openai.azure.com/\"\n",
    "api_key = \"\"\n",
    "api_version = \"2024-02-15-preview\"\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = azure_endpoint\n",
    "openai.api_key = api_key\n",
    "openai.api_version = api_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ada15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate embeddings for a given text\n",
    "def get_embedding(model, tokenizer, text, max_length=8192):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a transformer-based model.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        # Average the last hidden state across token dimensions to create the embedding\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba93a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "def clean_text_with_tags(text):\n",
    "    text_no_tags = re.sub(r'<\\/?.*?>', '', text)  # Remove <ata> and similar tags\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text_no_tags)  # Remove non-alphanumeric characters\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip().lower()  # Normalize spaces and convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_upra_data(datapath, debugging=False):\n",
    "    \"\"\"\n",
    "    Loads data from the provided Excel file and formats it for processing.\n",
    "    \"\"\"\n",
    "    upra_df = pd.read_excel(datapath)\n",
    "    if debugging:\n",
    "        print(\"Debugging Mode: Using few document samples only\")\n",
    "        upra_df = upra_df.head(5)\n",
    "    \n",
    "    data = []\n",
    "    with tqdm(total=len(upra_df), desc=\"Processing user profiles\") as pbar:\n",
    "        for _, row in upra_df.iterrows():\n",
    "            data.append(\n",
    "                {\n",
    "                    # 'review' will only have one atypical aspect tagged and only one label in 'A_prime'\n",
    "                    \"raw_chunk\": f\"U: {row['user_profile']}\\n\\nR: {row['reformulated_review_sentence']}\\n\\nOutput:\\n\\n{row['A_prime']}\",\n",
    "                    #\"raw_chunk\": f\"U: {row['user_profile']}\\n\\nR: {row['review']}\\n\\nOutput:\\n\\n{row['A_prime']}\",\n",
    "                    \"user_profile\": row['user_profile'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"business_id\": row['business_id'],\n",
    "                    \"review\": row['reformulated_review_sentence'],\n",
    "                    #\"review\": row['review'],\n",
    "                    \"true_strong_weak\": row['true_strong_weak'],\n",
    "                    \"abs_true_strong_weak\": row['abs_true_strong_weak'],\n",
    "                    \"output\": row['A_prime'],\n",
    "                    \"doc_id\": row.get(\"doc_id\", None),\n",
    "                    \"chunk_id\": row.get(\"chunk_id\", None),\n",
    "                    \"original_index\": row.get(\"original_index\", None),\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)\n",
    "    return data\n",
    "\n",
    "def precompute_embeddings_and_index(data, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Precompute embeddings for the data and index by A_prime labels.\n",
    "    \"\"\"\n",
    "    train_data_by_label = {\"None\": [], \"Low\": [], \"Medium\": [], \"High\": []}\n",
    "\n",
    "    for item in data:\n",
    "        # Enrich the existing dictionary with embeddings\n",
    "        item['vector_user_profile'] = get_embedding(model, tokenizer, clean_text_with_tags(item['user_profile']))\n",
    "        item['vector_abs_true_strong_weak'] = get_embedding(model, tokenizer, clean_text_with_tags(item['abs_true_strong_weak']))\n",
    "\n",
    "        # Index by A_prime label\n",
    "        if item['output'].endswith('\"None\")'):\n",
    "            train_data_by_label[\"None\"].append(item)\n",
    "        elif item['output'].endswith('\"Low\")'):\n",
    "            train_data_by_label[\"Low\"].append(item)\n",
    "        elif item['output'].endswith('\"Medium\")'):\n",
    "            train_data_by_label[\"Medium\"].append(item)\n",
    "        elif item['output'].endswith('\"High\")'):\n",
    "            train_data_by_label[\"High\"].append(item)\n",
    "    \n",
    "    '''print(\"Count of items in each label:\")\n",
    "    for label, items in train_data_by_label.items():\n",
    "        print(f\"{label}: {items} and count: {len(items)}\")'''\n",
    "\n",
    "    return train_data_by_label\n",
    "\n",
    "def calculate_harmonic_mean_and_top_k(test_query, train_data, top_k=1):\n",
    "    \"\"\"\n",
    "    Calculates the harmonic mean of similarities for each label group and retrieves top k examples.\n",
    "    \"\"\"\n",
    "    test_user_profile_embedding = get_embedding(model, tokenizer, clean_text_with_tags(test_query['user_profile']))\n",
    "    test_abs_true_strong_weak_embedding = get_embedding(model, tokenizer, clean_text_with_tags(test_query['abs_true_strong_weak']))\n",
    "    print(f\"Test-True Strong Weak: {test_query['true_strong_weak']}\")\n",
    "    few_shot_examples = []\n",
    "\n",
    "    #print(\"\\nProcessing Test Query:\")\n",
    "    #print(f\"User Profile: {test_query['user_profile']}\")\n",
    "    #print(f\"Review: {test_query['review']}\\n\")\n",
    "\n",
    "    for label, examples in train_data.items():\n",
    "        if examples:\n",
    "            print(f\"Processing Label: {label}\")\n",
    "            user_profile_similarities = cosine_similarity(\n",
    "                [test_user_profile_embedding],\n",
    "                [doc['vector_user_profile'] for doc in examples]\n",
    "            )[0]\n",
    "            abs_true_strong_weak_similarities = cosine_similarity(\n",
    "                [test_abs_true_strong_weak_embedding],\n",
    "                [doc['vector_abs_true_strong_weak'] for doc in examples]\n",
    "            )[0]\n",
    "            harmonic_mean_scores = hmean([user_profile_similarities, abs_true_strong_weak_similarities], axis=0)\n",
    "\n",
    "            #print(f\"Cosine Similarities (User Profile): {user_profile_similarities}\")\n",
    "            #print(f\"Cosine Similarities (Abs Strong Weak): {abs_true_strong_weak_similarities}\")\n",
    "            #print(f\"Harmonic Mean Scores: {harmonic_mean_scores}\")\n",
    "\n",
    "            # Get top-k examples\n",
    "            top_indices = harmonic_mean_scores.argsort()[-top_k:][::-1]\n",
    "            top_examples = [examples[i] for i in top_indices]\n",
    "            few_shot_examples.extend(top_examples)\n",
    "\n",
    "            print(f\"Top {top_k} Examples for Label '{label}':\")\n",
    "            for idx, example in enumerate(top_examples):\n",
    "                print(f\"Example {idx + 1}: {example['raw_chunk']} \\n Training-True Strong Weak: {example['true_strong_weak']} (Score: {harmonic_mean_scores[top_indices[idx]]})\\n\")\n",
    "\n",
    "    return few_shot_examples\n",
    "\n",
    "def process_results_to_excel(results, output_file):\n",
    "    \"\"\"\n",
    "    Process the results and save them to an Excel file.\n",
    "    \n",
    "    Parameters:\n",
    "        results (list): List of dictionaries with prediction results.\n",
    "        output_file (str): Path to save the output Excel file.\n",
    "    \"\"\"\n",
    "    corrected_results = []\n",
    "\n",
    "    # Filter out any entries with unexpected structure\n",
    "    for r in results:\n",
    "        if all(key in r for key in ['user_profile', 'name', 'business_id', 'review', 'predicted_output']):\n",
    "            corrected_results.append(r)\n",
    "        else:\n",
    "            print(f\"Skipping incorrect entry: {r}\")\n",
    "\n",
    "    results_df = pd.DataFrame(corrected_results, columns=['user_profile', 'name', 'business_id', 'review', 'predicted_output'])\n",
    "\n",
    "    # Function to parse predicted_output into A_prime and Explanation\n",
    "    def parse_predicted_output(result):\n",
    "        if isinstance(result, str):\n",
    "            if '\\n\\nExplanation: ' in result:\n",
    "                parts = result.split('\\n\\nExplanation: ', 1)\n",
    "                return [parts[0], parts[1]]\n",
    "            else:\n",
    "                return [result, '']\n",
    "        elif isinstance(result, list):\n",
    "            # If it's a list (e.g., multiple A' values), handle\n",
    "            a_prime = '; '.join(str(item) for item in result)\n",
    "            return [a_prime, '']\n",
    "        else:\n",
    "            # Handle unexpected formats\n",
    "            return [str(result), '']\n",
    "\n",
    "    results_df[['A_prime', 'Explanation']] = results_df['predicted_output'].apply(lambda x: pd.Series(parse_predicted_output(x)))\n",
    "\n",
    "    # Drop the original predicted_output column\n",
    "    results_df.drop(columns=['predicted_output'], inplace=True)\n",
    "\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    print(f\"Results successfully exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f633aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_params(user_profile, review):\n",
    "    \n",
    "    \"\"\"\n",
    "    Constructs the query-specific string for the prompt.\n",
    "    \"\"\"\n",
    "    prompt_params = f\"\"\"\n",
    "    \n",
    "    U: '{user_profile}'\n",
    "    \n",
    "    R: '{review}' \n",
    "\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    return prompt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(prompt, prompt_params, test_instance):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_params}\n",
    "        ]\n",
    "        \n",
    "        formatted_messages = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "        print(\"Messages being sent to GPT:\\n\", formatted_messages)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            messages=messages,\n",
    "            engine=\"gpt-4\",\n",
    "            temperature=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        result_content = response['choices'][0]['message']['content']\n",
    "        return {\n",
    "            \"user_profile\": test_instance['user_profile'],\n",
    "            \"name\": test_instance['name'],\n",
    "            \"business_id\": test_instance['business_id'],\n",
    "            \"review\": test_instance['review'],\n",
    "            \"abs_true_strong_weak\": test_instance['abs_true_strong_weak'],\n",
    "            \"predicted_output\": result_content,\n",
    "        }\n",
    "    except (InvalidRequestError, RateLimitError, APIConnectionError) as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        time.sleep(150) if isinstance(e, RateLimitError) else None\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b39b59",
   "metadata": {},
   "source": [
    "### Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8471a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt():\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/mapping/prompt_for_matching_restaurant_review_sentences_to_user_profiles_zs.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def zero_shot_inference(data):\n",
    "    system_labels = []\n",
    "    prompt = load_prompt()\n",
    "\n",
    "    for test_instance in data:\n",
    "        test_user_profile = test_instance['user_profile']\n",
    "        test_review = test_instance['review']\n",
    "\n",
    "        prompt_params = load_prompt_params(test_user_profile, test_review)\n",
    "\n",
    "        system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "\n",
    "        if system_label:\n",
    "            print(system_label['predicted_output'])\n",
    "            system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fa69b",
   "metadata": {},
   "source": [
    "### Fixed Few-Shot with CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8008fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_cot():\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/mapping/prompt_for_matching_restaurant_review_sentences_to_user_profiles.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "        with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/mapping/few_shot_examples_restaurants.txt', 'r') as fewshot_file:\n",
    "            few_shot_examples = fewshot_file.read()\n",
    "\n",
    "    return prompt.format(few_shot_examples=few_shot_examples)\n",
    "\n",
    "def few_shot_CoT_inference(data):\n",
    "    system_labels = []\n",
    "    prompt = load_prompt_cot()\n",
    "\n",
    "    for test_instance in data:\n",
    "        test_user_profile = test_instance['user_profile']\n",
    "        test_review = test_instance['review']\n",
    "\n",
    "        prompt_params = load_prompt_params(test_user_profile, test_review)\n",
    "\n",
    "        system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "\n",
    "        if system_label:\n",
    "            print(system_label['predicted_output'])\n",
    "            system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac0de1",
   "metadata": {},
   "source": [
    "### Few-Shot with RAG"
   ]
  },
  {
   "cell_type": "raw",
   "id": "731e4202",
   "metadata": {},
   "source": [
    "def load_prompt_rag(few_shot_examples):\n",
    "    # Read the prompt\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/mapping/prompt_for_matching_restaurants_to_user_profiles.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    return prompt.format(few_shot_examples=few_shot_examples)\n",
    "\n",
    "def loocv_inference2(data, model):\n",
    "    \"\"\"\n",
    "    Performs leave-one-out cross-validation (LOOCV) inference.\n",
    "    Ensures that for each test instance, no record in the training set shares the same user profile or review.\n",
    "    This approach minimizes the risk of data leakage by strictly filtering similar records.\n",
    "    \"\"\"\n",
    "    system_labels = []\n",
    "    \n",
    "    # Iterate over each record to perform LOOCV\n",
    "    for i, test_instance in enumerate(data):\n",
    "        print(f\"\\nProcessing Test Instance {i + 1} of {len(data)}...\")\n",
    "        \n",
    "        # Define the user profile and review of the current test instance\n",
    "        test_profile = clean_text_with_tags(test_instance['user_profile'])\n",
    "        test_review = clean_text_with_tags(test_instance['review'])\n",
    "        \n",
    "        # Filter the training data to exclude records with the same user profile or review\n",
    "        train_data = [item for j, item in enumerate(data) if j != i \\\n",
    "                      and clean_text_with_tags(item['user_profile']) != test_profile \\\n",
    "                      and clean_text_with_tags(item['review']) != test_review]\n",
    "        \n",
    "        print(f\"Number of records in training set: {len(train_data)}\")\n",
    "        print(f\"Excluding records with profile: {test_profile} and review: {test_review}\")\n",
    "        \n",
    "        # Precompute embeddings and index training data\n",
    "        train_data_by_label = precompute_embeddings_and_index(train_data, model, tokenizer)\n",
    "        \n",
    "        # Calculate few-shot examples using filtered training data\n",
    "        few_shot_examples = calculate_harmonic_mean_and_top_k(test_instance, train_data_by_label)\n",
    "        few_shot_formatted = \"\\n\\n\".join(\n",
    "            [f\"Example {k+1}:\\n\\n{doc['raw_chunk']}\" for k, doc in enumerate(few_shot_examples)]\n",
    "        )\n",
    "        \n",
    "        # Generate prompt for model\n",
    "        prompt = load_prompt_rag(few_shot_formatted)\n",
    "        prompt_params = load_prompt_params(test_instance['user_profile'], test_instance['review'])\n",
    "        system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "        \n",
    "        if system_label:\n",
    "            print(system_label['predicted_output'])\n",
    "            system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_rag(few_shot_examples):\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/mapping/prompt_for_matching_restaurant_review_sentences_to_user_profiles.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    return prompt.format(few_shot_examples=few_shot_examples)\n",
    "\n",
    "def loocv_inference(data, model):\n",
    "    \"\"\"\n",
    "    Performs leave-one-out cross-validation (LOOCV) inference.\n",
    "    Ensures that for each test instance, no record in the training set shares the same user profile or business_id.\n",
    "    This approach minimizes the risk of data leakage by strictly filtering similar records.\n",
    "    This updated version precomputes embeddings once before processing test instances.\n",
    "    \"\"\"\n",
    "    system_labels = []\n",
    "    \n",
    "    # Precompute embeddings and index training data before the loop\n",
    "    print(\"Precomputing embeddings for the entire dataset...\")\n",
    "    train_data_by_label = precompute_embeddings_and_index(data, model, tokenizer)\n",
    "    \n",
    "    # Iterate over each record to perform LOOCV\n",
    "    for i, test_instance in enumerate(data):\n",
    "        print(f\"\\nProcessing Test Instance {i + 1} of {len(data)}...\")\n",
    "        \n",
    "        # Define the user profile and review of the current test instance\n",
    "        test_profile = clean_text_with_tags(test_instance['user_profile'])\n",
    "        test_business_id = test_instance['business_id'].strip()\n",
    "\n",
    "        # Filter out records with the same user_profile OR same business_id\n",
    "        filtered_train_data_by_label = {\n",
    "            label: [\n",
    "                item for item in train_data_by_label[label]\n",
    "                if clean_text_with_tags(item['user_profile']) != test_profile\n",
    "                and item['business_id'].strip() != test_business_id\n",
    "            ]\n",
    "            for label in train_data_by_label\n",
    "        }\n",
    "        \n",
    "        print(f\"Number of records in training set after filtering: {sum(len(v) for v in filtered_train_data_by_label.values())}\")\n",
    "        print(f\"Excluding records with profile: {test_profile} and business id: {test_business_id}\")\n",
    "        \n",
    "        # Calculate few-shot examples using precomputed and filtered training data\n",
    "        few_shot_examples = calculate_harmonic_mean_and_top_k(test_instance, filtered_train_data_by_label)\n",
    "        few_shot_formatted = \"\\n\\n\".join(\n",
    "            [f\"Example {k+1}:\\n\\n{doc['raw_chunk']}\" for k, doc in enumerate(few_shot_examples)]\n",
    "        )\n",
    "        \n",
    "        prompt = load_prompt_rag(few_shot_formatted)\n",
    "        prompt_params = load_prompt_params(test_instance['user_profile'], test_instance['review'])\n",
    "        system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "        \n",
    "        if system_label:\n",
    "            print(system_label['predicted_output'])\n",
    "            system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da2738",
   "metadata": {},
   "source": [
    "### Zero-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "upra_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/ground_truth_w_split_abstractive_rag.xlsx\"\n",
    "data_to_process = load_upra_data(upra_path, debugging=False)\n",
    "results_zs = zero_shot_inference(data_to_process)\n",
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/system_user_profiles_restaurants_reviews_match_zs.xlsx\"\n",
    "process_results_to_excel(results_zs, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b3cac",
   "metadata": {},
   "source": [
    "### CoT Few-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d208167",
   "metadata": {},
   "outputs": [],
   "source": [
    "upra_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/ground_truth_w_split_abstractive_rag.xlsx\"\n",
    "data_to_process = load_upra_data(upra_path, debugging=False)\n",
    "results_cot = few_shot_CoT_inference(data_to_process)\n",
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/system_user_profiles_restaurants_reviews_match_cot.xlsx\"\n",
    "process_results_to_excel(results_cot, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbaab9",
   "metadata": {},
   "source": [
    "### RAG Leave-one-out CV Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad303497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upra_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/restaurants/ground_truth_w_split_abstractive_rag.xlsx\"\n",
    "data_to_process = load_upra_data(upra_path, debugging=False)\n",
    "results_rag = loocv_inference(data_to_process, model)\n",
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/mturk/mturk-marketplace-ready/test/system_results/hotels/system_user_profiles_restaurants_reviews_match_rag.xlsx\"\n",
    "process_results_to_excel(results_rag, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
