{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed6068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from scipy.stats import hmean\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import openai\n",
    "from openai.error import RateLimitError, APIConnectionError, InvalidRequestError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c4ce9",
   "metadata": {},
   "source": [
    "#### GPT-4 configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85867a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = \"https://rumi-gpt4.openai.azure.com/\"\n",
    "api_key = \"\"\n",
    "api_version = \"2024-02-15-preview\"\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = azure_endpoint\n",
    "openai.api_key = api_key\n",
    "openai.api_version = api_version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "702025a8",
   "metadata": {},
   "source": [
    "#### GPT-3.5 configuration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d812cf6",
   "metadata": {},
   "source": [
    "azure_endpoint = \"https://rumi-v0.openai.azure.com/\"\n",
    "api_key = \"\"\n",
    "api_version = \"2023-07-01-preview\"\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = azure_endpoint\n",
    "openai.api_key = api_key\n",
    "openai.api_version = api_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c311d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate embeddings for a given text\n",
    "def get_embedding(model, tokenizer, text, max_length=8192):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a transformer-based model.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        # Average the last hidden state across token dimensions to create the embedding\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf129139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews_data(datapath, debugging=False):\n",
    "    \"\"\"\n",
    "    Loads data from the provided Excel file and formats it for processing.\n",
    "    \"\"\"\n",
    "    reviews_df = pd.read_excel(datapath)\n",
    "    if debugging:\n",
    "        print(\"Debugging Mode: Using few document samples only\")\n",
    "        reviews_df = reviews_df.head(5)\n",
    "    \n",
    "    data = []\n",
    "    with tqdm(total=len(reviews_df), desc=\"Processing reviews\") as pbar:\n",
    "        for _, row in reviews_df.iterrows():\n",
    "            data.append(\n",
    "                {\n",
    "                    \"raw_chunk\": f\"{row['split_decomposed_review']}\\n\\nClassification: {row['manual_ata_classification']}\\nAtypical Aspects: {row['manual_ata_extractive']}\",\n",
    "                    \"name\": row['name'],  # Debugging field\n",
    "                    \"business_id\": row['business_id'],  # Debugging field\n",
    "                    \"review\": row['review'],\n",
    "                    \"d_review\": row['decomposed_review'],\n",
    "                    \"split_decomposed_review\": row['split_decomposed_review'],\n",
    "                    \"manual_ata_label\": row['manual_ata_classification'],\n",
    "                    \"manual_ext_aspects\": row['manual_ata_extractive'],\n",
    "                    \"true_strong_weak\": row['true_strong_weak'],\n",
    "                    #\"true_strong\": row['true_strong'],\n",
    "                    \"abs_true_strong_weak\": row['abs_true_strong_weak'],\n",
    "                    \"doc_id\": row.get(\"doc_id\", None),\n",
    "                    \"chunk_id\": row.get(\"chunk_id\", None),\n",
    "                    \"original_index\": row.get(\"original_index\", None),\n",
    "                }\n",
    "            )\n",
    "            pbar.update(1)\n",
    "    return data\n",
    "\n",
    "def process_results_to_excel(results, output_file):\n",
    "    \"\"\"\n",
    "    Process the results and save them to an Excel file.\n",
    "\n",
    "    Parameters:\n",
    "        results (list): List of dictionaries with prediction results.\n",
    "        output_file (str): Path to save the output Excel file.\n",
    "    \"\"\"\n",
    "    corrected_results = []\n",
    "\n",
    "    for r in results:\n",
    "        if all(key in r for key in ['name', 'business_id', 'review', 'split_decomposed_review', 'abs_true_strong_weak', 'predicted_output']):\n",
    "            # Extract and clean the \"Atypical aspects\"\n",
    "            atypical_aspects = r['predicted_output']\n",
    "            if \"Atypical:\" in atypical_aspects:\n",
    "                # Extract the part after \"Atypical:\" and strip whitespace\n",
    "                atypical_aspects = atypical_aspects.split(\"Atypical:\")[-1].strip()\n",
    "\n",
    "            # Append the cleaned result\n",
    "            corrected_results.append({\n",
    "                'name': r['name'],\n",
    "                'business_id': r['business_id'],\n",
    "                'review': r['review'],\n",
    "                'split_decomposed_review': r['split_decomposed_review'],\n",
    "                'abs_true_strong_weak': r['abs_true_strong_weak'],\n",
    "                'atypical_aspects': atypical_aspects,  # Cleaned output\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping incorrect entry: {r}\")\n",
    "\n",
    "    results_df = pd.DataFrame(corrected_results)\n",
    "\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    print(f\"Data successfully exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb034f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_params(review_sentence):\n",
    "    \n",
    "    \"\"\"\n",
    "    Constructs the query-specific string for the prompt.\n",
    "    \"\"\"\n",
    "    prompt_params = f\"\"\"\n",
    "    \n",
    "    {review_sentence}' \n",
    "\n",
    "    Atypical:\n",
    "    \"\"\"\n",
    "    return prompt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ff4b6",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(prompt, prompt_params, test_instance):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_params}\n",
    "        ]\n",
    "        \n",
    "        formatted_messages = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "        print(\"Messages being sent to GPT:\\n\", formatted_messages)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            messages=messages,\n",
    "            engine=\"gpt-4\",\n",
    "            temperature=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        result_content = response['choices'][0]['message']['content']\n",
    "        return {\n",
    "            \"name\": test_instance['name'],\n",
    "            \"business_id\": test_instance['business_id'],\n",
    "            \"review\": test_instance['review'],\n",
    "            \"split_decomposed_review\": test_instance['split_decomposed_review'],\n",
    "            \"abs_true_strong_weak\": test_instance['abs_true_strong_weak'],\n",
    "            \"predicted_output\": result_content,\n",
    "        }\n",
    "    \n",
    "    except (InvalidRequestError, RateLimitError, APIConnectionError) as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        time.sleep(150) if isinstance(e, RateLimitError) else None\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02c1095d",
   "metadata": {},
   "source": [
    "### GPT-3.5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6baa402e",
   "metadata": {},
   "source": [
    "def generate_model_response(prompt, prompt_params, test_instance):\n",
    "    try:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_params}\n",
    "        ]\n",
    "        \n",
    "        formatted_messages = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in messages])\n",
    "        print(\"Messages being sent to GPT:\\n\", formatted_messages)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            messages=messages,\n",
    "            engine=\"gpt-35-turbo\",\n",
    "            temperature=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        result_content = response['choices'][0]['message']['content']\n",
    "        return {\n",
    "            \"name\": test_instance['name'],\n",
    "            \"business_id\": test_instance['business_id'],\n",
    "            \"review\": test_instance['review'],\n",
    "            \"split_decomposed_review\": test_instance['split_decomposed_review'],\n",
    "            \"abs_true_strong_weak\": test_instance['abs_true_strong_weak'],\n",
    "            \"predicted_output\": result_content,\n",
    "        }\n",
    "    \n",
    "    except (InvalidRequestError, RateLimitError, APIConnectionError) as e:\n",
    "        print(f\"Error encountered: {e}\")\n",
    "        time.sleep(150) if isinstance(e, RateLimitError) else None\n",
    "    except Exception as e:\n",
    "        print(\"An unexpected error occurred:\", e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b726eb2",
   "metadata": {},
   "source": [
    "### Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7430e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt():\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/extracting/prompt_for_extractive_ata_classification_restaurants_zs.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "def zero_shot_inference(data):\n",
    "    \"\"\"\n",
    "    Perform zero-shot inference using all data without any cross-validation.\n",
    "    \"\"\"\n",
    "    print(\"Performing zero-shot inference using all data\")\n",
    "    system_labels = []\n",
    "\n",
    "    for test_instance in data:\n",
    "        test_sentence = test_instance['split_decomposed_review']\n",
    "\n",
    "        prompt = load_prompt()\n",
    "        prompt_params = load_prompt_params(test_sentence)\n",
    "\n",
    "        system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "\n",
    "        if system_label:\n",
    "            print(system_label['predicted_output'])\n",
    "            system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b136248",
   "metadata": {},
   "source": [
    "### Fixed Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_fs():\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/extracting/prompt_for_extractive_ata_classification_restaurants_fs.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "        with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/extracting/few_shot_for_extractive_ata_classification_restaurants.txt', 'r') as fewshot_file:\n",
    "            few_shot_examples = fewshot_file.read()\n",
    "        \n",
    "    return prompt.format(few_shot_examples=few_shot_examples)\n",
    "\n",
    "def fixed_few_shot_inference(data):\n",
    "    \"\"\"\n",
    "    Perform zero-shot inference using all data without any cross-validation.\n",
    "    \"\"\"\n",
    "    print(\"Performing zero-shot inference using all data\")\n",
    "    system_labels = []\n",
    "\n",
    "    for test_instance in data:\n",
    "        test_sentence = test_instance['split_decomposed_review']\n",
    "\n",
    "        prompt = load_prompt_fs()\n",
    "        prompt_params = load_prompt_params(test_sentence)\n",
    "\n",
    "        system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "\n",
    "        if system_label:\n",
    "            print(system_label['predicted_output'])\n",
    "            system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a7c1d",
   "metadata": {},
   "source": [
    "### RAG Few-Shot w/ labelwise examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_embeddings_and_index(data, model, tokenizer):\n",
    "    grouped_data = {\"<pos>\": [], \"<neg>\": []}\n",
    "    seen_sentences = {\"<pos>\": set(), \"<neg>\": set()}  # Track duplicates\n",
    "\n",
    "    for item in data:\n",
    "        sentence = item['split_decomposed_review'].strip()\n",
    "        label = item.get('manual_ata_label')\n",
    "\n",
    "        if sentence and label in grouped_data:\n",
    "            if sentence not in seen_sentences[label]:  # Avoid duplicates\n",
    "                embedding = get_embedding(model, tokenizer, clean_text(sentence))\n",
    "                item['sentence_embedding'] = embedding\n",
    "                grouped_data[label].append(item)\n",
    "                seen_sentences[label].add(sentence)\n",
    "\n",
    "    # Debugging: Print the number of examples per label\n",
    "    print(f\"Precomputed {len(grouped_data['<pos>'])} <pos> examples and {len(grouped_data['<neg>'])} <neg> examples.\\n\")\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "def calculate_top_k_examples(test_sentence, grouped_data, model, tokenizer, top_k=4):\n",
    "    \"\"\"\n",
    "    Calculate top-k similar examples for both <pos> and <neg> labels separately.\n",
    "    \"\"\"\n",
    "    test_embedding = get_embedding(model, tokenizer, clean_text(test_sentence))\n",
    "    top_k_results = {\"<pos>\": [], \"<neg>\": []}\n",
    "\n",
    "    for label in [\"<pos>\", \"<neg>\"]:\n",
    "        scores = []\n",
    "\n",
    "        print(f\"Calculating similarities for label: {label}\")\n",
    "        for item in grouped_data[label]:\n",
    "            embedding = item['sentence_embedding']\n",
    "            similarity = cosine_similarity([test_embedding], [embedding])[0][0]\n",
    "            scores.append((similarity, item))  # Store the full item here\n",
    "\n",
    "            # Debugging: Print similarity for each example\n",
    "            print(f\"Review: {item['split_decomposed_review'][:50]}... | Similarity: {similarity:.4f}\")\n",
    "\n",
    "        # Sort by similarity and select top-k\n",
    "        scores = sorted(scores, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "        top_k_results[label] = scores\n",
    "\n",
    "        # Debugging: Print top-k examples with similarity scores\n",
    "        print(f\"\\nTop-{top_k} examples for label '{label}':\")\n",
    "        for idx, (similarity, item) in enumerate(scores, 1):\n",
    "            print(f\"{idx}. Similarity: {similarity:.4f} | Sentence: {item['split_decomposed_review'][:50]}...\\n\")\n",
    "\n",
    "    return top_k_results\n",
    "\n",
    "def load_prompt_rag(few_shot_examples):\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/extracting/prompt_for_extractive_ata_classification_restaurants_fs.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    return prompt.format(few_shot_examples=few_shot_examples)\n",
    "\n",
    "def logocv_business_grouped_inference(data, model):\n",
    "    \"\"\"\n",
    "    Perform inference using Leave-One-Group-Out Cross-Validation (LOGO-CV),\n",
    "    grouping data by `business_id`.\n",
    "    \"\"\"\n",
    "    print(\"Performing inference using Leave-One-Group-Out Cross-Validation (LOGO-CV).\")\n",
    "    system_labels = []\n",
    "\n",
    "    # Extract groups (business_id) from the data\n",
    "    groups = [item['business_id'] for item in data]\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(logo.split(data, groups=groups)):\n",
    "        test_business_id = groups[test_indices[0]]\n",
    "        print(f\"\\nProcessing Fold {fold_idx + 1}: Testing on Business ID {test_business_id}...\")\n",
    "\n",
    "        train_data = [data[i] for i in train_indices]\n",
    "        test_data = [data[i] for i in test_indices]\n",
    "\n",
    "        # Precompute embeddings and group by <pos> and <neg>\n",
    "        grouped_data = precompute_embeddings_and_index(train_data, model, tokenizer)\n",
    "\n",
    "        for test_instance in test_data:\n",
    "            test_sentence = test_instance['split_decomposed_review']\n",
    "            print(f\"\\nProcessing Test Sentence: {test_sentence[:50]}...\\n\")\n",
    "\n",
    "            # Calculate top 4 similar examples for <pos> and <neg>\n",
    "            top_k_matches = calculate_top_k_examples(test_sentence, grouped_data, model, tokenizer)\n",
    "\n",
    "            # Prepare few-shot examples directly from top_k_matches\n",
    "            few_shot_examples = {\"<pos>\": [], \"<neg>\": []}\n",
    "\n",
    "            for label in top_k_matches:\n",
    "                for similarity, item in top_k_matches[label]:\n",
    "                    few_shot_examples[label].append(item['raw_chunk'])  # Direct access\n",
    "\n",
    "                while len(few_shot_examples[label]) < 4:\n",
    "                    few_shot_examples[label].append(f\"Placeholder example for {label}\")\n",
    "\n",
    "            few_shot_formatted = \"\\n\\n\".join(\n",
    "                [f\"Example {i+1}:\\n\\n{chunk}\" for i, chunk in enumerate(few_shot_examples[\"<neg>\"] + few_shot_examples[\"<pos>\"])]\n",
    "            )\n",
    "\n",
    "            prompt = load_prompt_rag(few_shot_formatted)\n",
    "            prompt_params = load_prompt_params(test_sentence)\n",
    "\n",
    "            system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "\n",
    "            if system_label:\n",
    "                print(f\"Predicted Output: {system_label['predicted_output']}\\n\")\n",
    "                system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e619e5",
   "metadata": {},
   "source": [
    "### RAG Few-Shot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e24d7f8",
   "metadata": {},
   "source": [
    "def precompute_embeddings_and_index(data, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Precompute embeddings for `split_decomposed_review` in the training data.\n",
    "    \"\"\"\n",
    "    for item in data:\n",
    "        # Generate embeddings directly for the sentence in `split_decomposed_review`\n",
    "        sentence = item['split_decomposed_review']\n",
    "        if sentence.strip():\n",
    "            embedding = get_embedding(model, tokenizer, clean_text(sentence))\n",
    "            # Store sentence and embedding\n",
    "            item['sentence_embeddings'] = [(sentence, embedding)]\n",
    "    return data\n",
    "\n",
    "def calculate_top_k_examples(test_sentence, train_data, model, tokenizer, top_k=8):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity for the test sentence (`split_decomposed_review`) \n",
    "    with `split_decomposed_review` sentences in the training data.\n",
    "    \"\"\"\n",
    "    test_embedding = get_embedding(model, tokenizer, clean_text(test_sentence))\n",
    "    scores = []\n",
    "\n",
    "    # Iterate through the training data and compute similarity\n",
    "    for item in train_data:\n",
    "        for sentence, embedding in item['sentence_embeddings']:\n",
    "            similarity = cosine_similarity([test_embedding], [embedding])[0][0]\n",
    "            scores.append((similarity, sentence, item['review']))\n",
    "\n",
    "    # Sort scores in descending order of similarity and select the top k\n",
    "    scores = sorted(scores, key=lambda x: x[0], reverse=True)\n",
    "    top_k_matches = []\n",
    "    seen_reviews = set()\n",
    "\n",
    "    for score, sentence, review in scores:\n",
    "        if review not in seen_reviews:\n",
    "            top_k_matches.append((sentence, review, score))\n",
    "            seen_reviews.add(review)\n",
    "        if len(top_k_matches) == top_k:\n",
    "            break\n",
    "\n",
    "    return top_k_matches\n",
    "\n",
    "def load_prompt_rag(few_shot_examples):\n",
    "    \"\"\"\n",
    "    Reads and formats the prompt template with the few-shot examples.\n",
    "    \"\"\"\n",
    "    with open('/Users/innerpiece92/Desktop/NLP_Workspace/AArec/prompt/extracting/prompt_for_extractive_ata_classification_restaurants_fs.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    return prompt.format(few_shot_examples=few_shot_examples)\n",
    "\n",
    "def logocv_business_grouped_inference2(data, model):\n",
    "    \"\"\"\n",
    "    Perform Leave-One-Group-Out Cross-Validation (LOGO-CV) inference,\n",
    "    grouping data by `business_id`.\n",
    "    \"\"\"\n",
    "    print(\"Using Leave-One-Group-Out Cross-Validation (LOGO-CV) with `business_id`.\")\n",
    "    system_labels = []\n",
    "\n",
    "    # Extract groups (business_id) from the data\n",
    "    groups = [item['business_id'] for item in data]  # Grouping key\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(logo.split(data, groups=groups)):\n",
    "        test_business_id = groups[test_indices[0]]  # Business ID being tested\n",
    "        print(f\"\\nProcessing Fold {fold_idx + 1}: Testing on Business ID {test_business_id}...\")\n",
    "\n",
    "        # Split train and test data based on indices\n",
    "        train_data = [data[i] for i in train_indices]\n",
    "        test_data = [data[i] for i in test_indices]\n",
    "\n",
    "        # Precompute embeddings for training data\n",
    "        precompute_embeddings_and_index(train_data, model, tokenizer)\n",
    "\n",
    "        for test_instance in test_data:\n",
    "            test_sentence = test_instance['split_decomposed_review']\n",
    "\n",
    "            # Calculate top k similar examples\n",
    "            top_k_matches = calculate_top_k_examples(test_sentence, train_data, model, tokenizer)\n",
    "\n",
    "            # Prepare few-shot examples using `raw_chunk`\n",
    "            few_shot_examples = []\n",
    "            for sentence, review, _ in top_k_matches:\n",
    "                for item in train_data:\n",
    "                    if item['review'] == review:\n",
    "                        few_shot_examples.append(item['raw_chunk'])\n",
    "                        break  # Avoid duplicates\n",
    "\n",
    "            # Combine and format few-shot examples\n",
    "            few_shot_formatted = \"\\n\\n\".join(\n",
    "                [f\"Example {i+1}:\\n\\n{chunk}\" for i, chunk in enumerate(few_shot_examples)]\n",
    "            )\n",
    "\n",
    "            # Load and format the prompt\n",
    "            prompt = load_prompt_rag(few_shot_formatted)\n",
    "            prompt_params = load_prompt_params(test_sentence)\n",
    "\n",
    "            # Generate system response\n",
    "            system_label = generate_model_response(prompt, prompt_params, test_instance)\n",
    "\n",
    "            if system_label:\n",
    "                print(system_label['predicted_output'])\n",
    "                system_labels.append(system_label)\n",
    "\n",
    "    return system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0ff95",
   "metadata": {},
   "source": [
    "### Zero-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a955753",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/restaurant_reviews_split_ata_classified.xlsx\"\n",
    "data_to_process = load_reviews_data(reviews_path, debugging=False)\n",
    "results_zs = zero_shot_inference(data_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/system_abstractive_ata_restaurants_reviews_zs.xlsx\"\n",
    "process_results_to_excel(results_zs, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1576ece",
   "metadata": {},
   "source": [
    "### Fixed Few-Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/restaurant_reviews_split_ata_classified.xlsx\"\n",
    "data_to_process = load_reviews_data(reviews_path, debugging=False)\n",
    "results_fs = fixed_few_shot_inference(data_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/system_abstractive_ata_restaurants_reviews_fs.xlsx\"\n",
    "process_results_to_excel(results_fs, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c240117",
   "metadata": {},
   "source": [
    "### Leave-One-Group-Out Inference w/ labelwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd75a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/pr_restaurant_reviews_split_ata_classified.xlsx\"\n",
    "data_to_process = load_reviews_data(reviews_path, debugging=False)\n",
    "results_logocv = logocv_business_grouped_inference(data_to_process, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/pr_system_abstractive_ata_restaurants_reviews_rag.xlsx\"\n",
    "process_results_to_excel(results_logocv, output_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6cd1331",
   "metadata": {},
   "source": [
    "### Leave-One-Group-Out Inference"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c934a28",
   "metadata": {},
   "source": [
    "reviews_path = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification/restaurant_reviews_split_ata_classified.xlsx\"\n",
    "data_to_process = load_reviews_data(reviews_path, debugging=False)\n",
    "results_logocv = logocv_business_grouped_inference2(data_to_process, model)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad1e0535",
   "metadata": {},
   "source": [
    "#output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/system_results_RAG/system_extracting_ata_restaurants_reviews.xlsx\"\n",
    "output_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v2/system_abstractive_ata_restaurants_reviews_rag2.xlsx\"\n",
    "process_results_to_excel(results_logocv, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
