{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(nlp.pipe_names)\n",
    "from extractor import BNPExtractor\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e105693",
   "metadata": {},
   "source": [
    "### Exact Match Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698384d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by applying Unicode normalization, lowercasing, and stripping spaces.\"\"\"\n",
    "    return unicodedata.normalize('NFKC', text.lower().strip())\n",
    "\n",
    "def calculate_metrics_exact_match(labels, preds):\n",
    "    true_pos = 0\n",
    "    gold_total = 0\n",
    "    pred_total = 0\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    \n",
    "    # Loop over both manual labels (ground truth) and system predictions\n",
    "    for golds, pred in zip(labels, preds):\n",
    "        print(f'GOLDs: {golds}, PREDs: {pred}')\n",
    "        \n",
    "        # Convert to lowercase, normalize Unicode, and split by \", \" to create sets\n",
    "        golds_set = set(map(normalize_text, golds.split(\", \"))) if golds else set()\n",
    "        pred_set = set(map(normalize_text, pred.split(\", \"))) if pred else set()\n",
    "        \n",
    "        gold_total += len(golds_set)\n",
    "        pred_total += len(pred_set)\n",
    "        \n",
    "        # Initialize a counter for true positives\n",
    "        num_gold_in_pred = 0\n",
    "        \n",
    "        # Check for matching values and print the gold values that are found in predictions\n",
    "        for aspect in golds_set:\n",
    "            if aspect in pred_set:\n",
    "                num_gold_in_pred += 1\n",
    "                print(f\"Matched gold aspect: {aspect}\")\n",
    "            else:\n",
    "                print(f\"Unmatched gold aspect: {aspect}\")\n",
    "        \n",
    "        true_pos += num_gold_in_pred\n",
    "        print(f'num_gold_in_pred: {num_gold_in_pred}, gold len: {len(golds_set)}, pred len: {len(pred_set)}, true pos: {true_pos}')\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    if true_pos:\n",
    "        recall = 100 * (true_pos / gold_total)\n",
    "        precision = 100 * (true_pos / pred_total) if pred_total > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {'precision': round(precision, 2), 'recall': round(recall, 2), 'f1': round(f1, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8caf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3_gpt35_pr/restaurant_reviews_split_ata_classified.xlsx\"\n",
    "manual_df = pd.read_excel(manual_file)\n",
    "system_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3_gpt35_pr/system_abstractive_ata_restaurants_reviews_rag.xlsx\"\n",
    "system_df = pd.read_excel(system_file)\n",
    "\n",
    "manual_labels = manual_df['manual_ata_extractive'].fillna('')\n",
    "system_preds = system_df['atypical_aspects'].fillna('')\n",
    "\n",
    "metrics = calculate_metrics_exact_match(manual_labels, system_preds)\n",
    "print(f\"Precision: {metrics['precision']}%\")\n",
    "print(f\"Recall: {metrics['recall']}%\")\n",
    "print(f\"F1 Score: {metrics['f1']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf046b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the metrics to an Excel file\n",
    "export_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/exact_match_evaluation_metrics_rag.xlsx\"\n",
    "export_df = pd.DataFrame([metrics])\n",
    "export_df.to_excel(export_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b4ee67",
   "metadata": {},
   "source": [
    "### Partial Match Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d587076",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = BNPExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestMatchedPhrase(phrase_1_tokens, phrase_list):\n",
    "    jaccard_sim_list = []\n",
    "    for phrase_2_tokens in phrase_list:\n",
    "        intersection = phrase_1_tokens.intersection(phrase_2_tokens)\n",
    "        jaccard_sim = len(intersection) / len(phrase_1_tokens.union(phrase_2_tokens))\n",
    "        jaccard_sim_list.append(jaccard_sim)\n",
    "    if max(jaccard_sim_list) > 0:\n",
    "        return phrase_list[jaccard_sim_list.index(max(jaccard_sim_list))]\n",
    "    return ''\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score with partial matching logic\n",
    "def calculate_metrics_exact_match_with_partial(preds, labels, tokenize=False):\n",
    "    true_pos_e, true_pos_g = 0, 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    \n",
    "    # Loop over both manual labels (ground truth) and system predictions\n",
    "    for golds, pred in zip(labels, preds):\n",
    "        ep_list = []\n",
    "        gp_list = []\n",
    "        \n",
    "        # Tokenize and clean up predictions and gold labels\n",
    "        print(f\"\\nGOLDs: {golds} \\nPREDs: {pred}\")\n",
    "        \n",
    "        if pred:\n",
    "            pred = pred.strip().split(\", \")\n",
    "            for ep in pred:\n",
    "                # Use extractor.tokenize() for tokenization and normalize\n",
    "                ep_list.append(set(normalize_text(token.text) for token in extractor.tokenize(ep)))\n",
    "            print(f\"Tokenized prediction phrases (ep_list): {ep_list}\")\n",
    "        \n",
    "        if golds:\n",
    "            gold_pos = set(golds.split(\", \"))\n",
    "            for gp in gold_pos:\n",
    "                # Use extractor.tokenize() for tokenization and normalize\n",
    "                gp_list.append(set(normalize_text(token.text) for token in extractor.tokenize(gp)))\n",
    "            print(f\"Tokenized ground truth phrases (gp_list): {gp_list}\")\n",
    "        \n",
    "        # Exact match case: no golds and no preds\n",
    "        if not ep_list and not gp_list:\n",
    "            true_pos_e += 1\n",
    "            true_pos_g += 1\n",
    "        \n",
    "        # False positive: system predicts aspects but none exist in gold\n",
    "        if ep_list and not gp_list:\n",
    "            false_pos += 1\n",
    "        \n",
    "        # False negative: gold has aspects but none predicted\n",
    "        if not ep_list and gp_list:\n",
    "            false_neg += 1\n",
    "\n",
    "        bestMatchedEP, bestMatchedGP = {}, {}\n",
    "        \n",
    "        # Find the best match for each gold phrase\n",
    "        for gp_tokens in gp_list:\n",
    "            if ep_list:\n",
    "                bestMatchedEP[tuple(gp_tokens)] = getBestMatchedPhrase(gp_tokens, ep_list)\n",
    "                print(f\"Best matched predicted phrase for gold '{gp_tokens}': {bestMatchedEP[tuple(gp_tokens)]}\")\n",
    "        \n",
    "        # Find the best match for each predicted phrase\n",
    "        for ep_tokens in ep_list:\n",
    "            if gp_list:\n",
    "                bestMatchedGP[tuple(ep_tokens)] = getBestMatchedPhrase(ep_tokens, gp_list)\n",
    "                print(f\"Best matched gold phrase for predicted '{ep_tokens}': {bestMatchedGP[tuple(ep_tokens)]}\")\n",
    "        \n",
    "        # Match gold to predicted, calculate partial match stats\n",
    "        for gp_tokens in gp_list:\n",
    "            if ep_list:\n",
    "                matched_EP = bestMatchedEP[tuple(gp_tokens)]\n",
    "                if matched_EP and bestMatchedGP[tuple(matched_EP)] == gp_tokens:\n",
    "                    ep_tokens = matched_EP\n",
    "                else:\n",
    "                    ep_tokens = ''\n",
    "                \n",
    "                # Partial matching\n",
    "                if ep_tokens:\n",
    "                    print(f\"Partial match found between: {ep_tokens} and {gp_tokens}\")\n",
    "                    true_pos_e += len(ep_tokens.intersection(gp_tokens)) / len(ep_tokens)\n",
    "                    true_pos_g += len(ep_tokens.intersection(gp_tokens)) / len(gp_tokens)\n",
    "                    false_pos += len(ep_tokens - gp_tokens) / len(ep_tokens)\n",
    "                    false_neg += len(gp_tokens - ep_tokens) / len(gp_tokens)\n",
    "                    ep_list.remove(ep_tokens)\n",
    "                else:\n",
    "                    print(f\"No match found for gold phrase '{gp_tokens}'\")\n",
    "                    false_neg += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "\n",
    "        if ep_list:\n",
    "            # No prediction to match with gold\n",
    "            print(f\"Unmatched prediction phrases remaining: {ep_list}\")\n",
    "            false_pos += len(ep_list)\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    if true_pos_e or true_pos_g:\n",
    "        precision = 100 * true_pos_e / (true_pos_e + false_pos)\n",
    "        recall = 100 * true_pos_g / (true_pos_g + false_neg)\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"\\nFinal counts: true_pos_e = {true_pos_e}, true_pos_g = {true_pos_g}, false_pos = {false_pos}, false_neg = {false_neg}\")\n",
    "    return {'precision': round(precision, 2), 'recall': round(recall, 2), 'f1': round(f1, 2)}\n",
    "\n",
    "metrics_partial_m = calculate_metrics_exact_match_with_partial(system_preds, manual_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Precision: {metrics_partial_m['precision']}%\")\n",
    "print(f\"Recall: {metrics_partial_m['recall']}%\")\n",
    "print(f\"F1 Score: {metrics_partial_m['f1']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the metrics to an Excel file\n",
    "export_file = \"/Users/innerpiece92/Desktop/NLP_Workspace/AArec/datasets/extracting_atypical_aspects_of_items_from_reviews/restaurants/test/rcb_using_decomposed_review_classification_v3/partial_match_evaluation_metrics_rag.xlsx\"\n",
    "export_df = pd.DataFrame([metrics_partial_m])\n",
    "export_df.to_excel(export_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1afd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
